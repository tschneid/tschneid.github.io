<!DOCTYPE HTML>
<!--
	Prologue 1.2 by HTML5 UP
	html5up.net | @n33co
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Timo Schneider</title>
		<meta http-equiv="content-type" content="text/html; charset=utf-8" />
		<meta name="description" content="" />
		<meta name="keywords" content="" />
		<meta name="MobileOptimized" content="640">
		<meta name="HandheldFriendly" content="True">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, target-densityDpi=device-dpi">
	
		<link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:200,300,400,600" rel="stylesheet" type="text/css" />
		<!--[if lte IE 8]><script src="js/html5shiv.js"></script><![endif]-->
		<script src="js/jquery.min.js"></script>
		<script src="js/skel.js"></script>
		<script src="js/skel-panels.min.js"></script>
		<script src="js/init.js"></script>
		<script type="text/javascript">
		$(document).ready(function () {
			$(".button.publication.details").on("click", function(e) {
			  e.preventDefault();  // prevent navigating
			  var selector = $(this).data("toggle");  // get corresponding selector from data-toggle
			  $(selector).slideToggle(400);
			});
		});
	</script>
		<noscript>
			<link rel="stylesheet" href="css/skel-noscript.css" />
			<link rel="stylesheet" href="css/style.css" />
			<link rel="stylesheet" href="css/style-wide.css" />
		</noscript>
		<!--[if lte IE 9]><link rel="stylesheet" href="css/ie9.css" /><![endif]-->
		<!--[if lte IE 8]><link rel="stylesheet" href="css/ie8.css" /><![endif]-->
	</head>
	<body>

		<!-- Header -->
			<div id="header" class="skel-panels-fixed">

				<div class="top">

					<!-- Logo -->
						<div id="logo">
							<span class="image avatar128 circular"><img src="images/avatar.jpg" alt="" /></span>
							<h1>Timo Schneider</h1>
							<span class="byline">
							<script type="text/javascript">
			        <!--
				        var subheading = new Array();
				        subheading[0] = 'Trust me, I\'m an engineer!';
				        subheading[1] = 'Here be dragons.';
				        subheading[2] = 'It\'s kind of fun to do the impossible</br>(Walt Disney)';

				        var ridx = Math.round(Math.random()*(subheading.length-1));
				        document.write(subheading[ridx]);
			        // -->
			        </script> 
							</span>
						</div>

					<!-- Nav -->
						<nav id="nav">
							<!--
							
								Prologue's nav expects links in one of two formats:
								
								1. Hash link (scrolls to a different section within the page)
								
								   <li><a href="#foobar" id="foobar-link" class="skel-panels-ignoreHref"><span class="fa fa-whatever-icon-you-want">Foobar</span></a></li>

								2. Standard link (sends the user to another page/site)

								   <li><a href="http://foobar.tld"><span class="fa fa-whatever-icon-you-want">Foobar</span></a></li>
							
							-->
							<ul>
								<li><a href="#about" id="about-link" class="skel-panels-ignoreHref"><span class="fa fa-user">About Me</span></a></li>
								<li><a href="#publications" id="publications-link" class="skel-panels-ignoreHref"><span class="fa fa-list-ul">Publications</span></a></li>
								<li><a href="#photography" id="photography-link" class="skel-panels-ignoreHref"><span class="fa fa-camera">Photography</span></a></li>
								<li><a href="#contact" id="contact-link" class="skel-panels-ignoreHref"><span class="fa fa-envelope">Contact</span></a></li>
							</ul>
						</nav>
						
				</div>
				
				<div class="bottom">

					<!-- Social Icons -->
						<ul class="icons">
							<!--<li><a href="#" class="fa fa-twitter solo"><span>Twitter</span></a></li>--> 
							<!--<li><a href="#" class="fa fa-facebook solo"><span>Facebook</span></a></li> Sorry, I'm not on facebook ;) -->
							<li><a href="https://github.com/tschneid" class="fa fa-github solo"><span>Github</span></a></li>
							<li><a href="https://plus.google.com/+TimoSchneider1" class="fa fa-google-plus solo"><span>Google+</span></a></li>
							<li><a href="https://www.xing.com/profile/Timo_Schneider82" class="fa fa-xing solo"><span>Xing</span></a></li>
							<li><a href="https://www.linkedin.com/profile/view?id=230909248" class="fa fa-linkedin solo"><span>LinkedIn</span></a></li>
							<li><a href="mailto:timo.schneider@student.kit.edu" class="fa fa-envelope solo"><span>Email</span></a></li>
						</ul>
				
				</div>
			
			</div>

		<!-- Main -->
			<div id="main">

				<!-- About Me -->
					<section id="about" class="one">
						<div class="container">

							<!--<span class="image featured"><img src="images/featured.jpg" alt="" /></span>-->
							<!--<span class="image featured"><img class="image featured" src="images/featured.jpg" alt="" /></span>-->

							<!--<div class="only-wide">"wide" is active.</div>
							<div class="only-normal">"normal" is active.</div>
					    <div class="only-narrow">"narrow" is active.</div>
 					    <div class="only-narrower">"narrower" is active.</div>
					    <div class="only-mobile">"mobile" is active.</div>-->


							</br>
							<header>
								<h2>Hi, I'm <strong>Timo</strong>!</h2>
							</header>

							<div class="about-container">

							<span class="publication image"><img src="images/mebackpack.jpg" /></span>
							
							<p>I'm a research assistant at the <a href="https://cvhci.anthropomatik.kit.edu/">Computer Vision for Human-Computer Interaction</a> (CVHCI) Lab, <a href="https://www.kit.edu">Karlsruhe Institute of Technology</a>, Germany, where I completed my Diploma (MSc) in Informatics in 2014.</p>
							<p> My research interests include Computer Vision, Machine Learning, and especially camera-based Gaze Estimation (or Eye Tracking).</p>
							<p>If you have any questions or requests, please feel free to <a href="#contact" id="contact-link2" class="skel-panels-ignoreHref">contact</a> me.</p>
							</div>
							

						</div>
					</section>
			
				

			<!-- Publications -->
					<section id="publications" class="two">
						<div class="container">

							<header>
								<h2>Publications</h2>
							</header>
							<div class="publication-copyrights">
							<strong>Copyright Notice:</strong> This material is presented to ensure timely dissemination of scholarly and technical work. Copyright and all rights therein are retained by authors or by other copyright holders. All persons copying this information are expected to adhere to the terms and constraints invoked by each author's copyright. In most cases, these works may not be reposted without the explicit permission of the copyright holder.
							</div>

							<ul>

							<!-- LIST OF PUBLICATIONS -->
							
							<li class="publication">
						  <div class="publication-container">
							  <a href="images/papers/schneider2014manifold.png"><span class="publication image"><img src="images/papers/schneider2014manifold_small.jpg" /></span></a>
						    <h4 class="publication title">Manifold Alignment for Person Independent Appearance-based Gaze Estimation</h4>
						    <span class="publication description"/>Timo Schneider, Boris Schauerte, Rainer Stiefelhagen <br/>	International Conference on Pattern Recognition (ICPR), Stockholm, Sweden, August, 2014.</span>
						    <p>
						      <a type="button" class="publication button" href="https://cvhci.anthropomatik.kit.edu/~tschneid/pdf/schneider2014manifold.pdf">PDF</a>
						      <a type="button" class="publication button" href="https://cvhci.anthropomatik.kit.edu/~tschneid/bibtex/schneider2014manifold.bib">Bib</a>
						      <a type="button" class="publication button details" href="#" data-toggle="#schneider2014manifold_details">Details</a>
						    </p>
						    <p>
					      <div class="publication-details" id="schneider2014manifold_details">
					        <strong>Abstract: </strong>We show that dually supervised manifold embedding can improve the performance of machine learning based person-independent and thus calibration-free gaze estimation. For this purpose, we perform a manifold embedding for each person in the training dataset and then learn a linear transformation that aligns the individual, person-dependent manifolds. We evaluate the effect of manifold alignment on the recently presented Columbia dataset, where we analyze the influence on 6 regression methods and 8 feature variants. Using manifold alignment, we are able to improve the person-independent gaze estimation performance by up to 31.2% compared to the best approach without manifold alignment.</br>
					        <strong>Keywords: </strong>Gaze Estimation, Manifold Alignment, Calibration-Free, Machine Learning</br>
					      </div>
						    </p>
						  </div>
						  </li>

							<li class="publication">
						  <div class="publication-container">
							  <a href="images/papers/schneider2014appearance.jpg"><span class="publication image"><img src="images/papers/schneider2014appearance_small.jpg" /></span></a>
						    <h4 class="publication title">Appearance-based Gaze Estimation using Manifold Alignment and Image Synthesis</h4>
						    <span class="publication description"/>Timo Schneider<br/>Diploma Thesis (MSc), Karlsruhe Institute of Technology, January, 2014</span>
						    <p>
						      <!--<a type="button" class="publication button" href="#">PDF</a>
						      <a type="button" class="publication button" href="#">Bib</a>-->
						      <a type="button" class="publication button details" href="#" data-toggle="#schneider2014appearance_details">Details</a>
						    </p>
						    <p>
					      <div class="publication-details" id="schneider2014appearance_details">
					        <p><strong>Abstract: </strong>
					        Vision-based gaze estimation promises to be a cheap and accessible alternative to
									dedicated gaze tracking hardware. Since cameras are ubiquitous in our personal
									computing devices, like laptops, smartphones, and tablets, this offers opportunities
									for a plethora of new applications. By utilizing gaze behavior as another natural
									means of non-verbal communication, we can foster more convenient and natural
									human-computer interaction.</p>
									<p>Remote gaze estimation does not need any physical contact with the user. However,
									it requires a calibration step, in which the user has to look at distinct calibration
									points, which in turn demands cooperation and effort. By omitting this calibration
									and using data of other calibrated persons, the estimation precision notably suffers,
									due to the person-specific eye appearances. As a solution, we propose Synchronized
									Delaunay Submanifold Embedding (SDSE), a manifold alignment technique that
									adjusts the persons’ eye patch features with similar gaze directions, to form a person-
									independent manifold. Thereby, we overcome the problem of dissimilar appearances
									and improve the performance for the subsequent calibration-free gaze estimation.
									Our tests with SDSE are conducted on two different gaze datasets, gawKIT and
									Columbia, for which we outperform the baseline by 10 % and 24 %, respectively.</p>
									<p>Comprehensive training data is the key to build robust regression models. We use
									the available data to synthesize new eye images by using optical flow for image in-
									terpolation. This method non-linearly expands the existing manifolds and therefore
									provides supplemental information for training regression models. Here, we cannot
									beat the baseline without synthesized images, however, we decrease the estimation
									error for most configurations – e.g., Histograms of Oriented Gradients (HOG) in com-
									bination with k Nearest Neighbors (kNN) by 30 %, or multilevel-HOG with Principal
									Component Analysis (PCA) and Support Vector Regression (SVR) by 10 %.</p>
									<p>The gaze estimation performance is tested on combinations of prominent features
									and regression methods, plus low-dimensional embedding by PCA. Amongst oth-
									ers, we compare the raw image, HOG and multilevel-HOG, Local Binary Patterns
									(LBP), or Discrete Cosine Transform (DCT) as features, and reduce the dimen-
									sionality to 256, 128, 64, 32, 16, and 8 dimensions, respectively. For regression we
									employ the aforementioned SVR and kNN, as well as Gaussian Processes Regression
									(GPR), Relevance Vector Regression (RVR), Regression Trees (Regtrees), Partial
									Least Squares (PLS), and Multivariate Adaptive Regression Splines (Splines). Be-
									sides, we use statistical tests to compare the results with and without SDSE, as
									well as with and without image synthesis, to ensure that the measured performance
									improvement is statistical significant. The observed improvement in the majority
									of combinations encourages the future use of the proposed approaches for precise
									calibration-free gaze estimation.</p>
					        <strong>Keywords: </strong>Gaze Estimation, Manifold Alignment, Image Synthesis, Calibration-Free, Machine Learning</br>
					      </div>
						    </p>
						  </div>
						  </li>

							<li class="publication">
						  <div class="publication-container">
							  <a href="images/papers/schneider2014viewing.jpg"><span class="publication image"><img src="images/papers/schneider2014viewing_small.jpg" /></span></a>

						    <h4 class="publication title">Viewing Direction Estimation Based on 3-D Eyeball Construction for HRI</h4>
						    <span class="publication description"/>Timo Schneider<br/>Study Thesis, Karlsruhe Institute of Technology, February, 2012</span>
						    <p>
						      <a type="button" class="publication button details" href="#" data-toggle="#schneider2014viewing_details">Details</a>
						    </p>

						    <p>
					      <div class="publication-details" id="schneider2014viewing_details">
					        <p><strong>Abstract: </strong>
					        Gaze estimation refers to the task of estimating the point in 3-D space, at which the user
									is looking at, and indicates where his mental focus lies on. Alternatively the visual axis
									and the 3-D position of the user’s pupil can be calculated.</p>
									<p>For this purpose, numerous approaches have been developed over the past three decades,
									and are required for various fields of application—from human-computer interfaces to
									psychological studies. Current commercial systems primarily use infrared light emitting
									diodes and image sensors to determine the position and orientation of the eye. But,
									this techniques require expensive hardware and can often not be used under uncontrolled
									lighting conditions, e.g., outdoors.</p>
									<p>Gaze estimation can be utilized for a more natural and convenient human-robot interaction.
									The behaviour of our eyes plays an important role in natural conversation. For example,
									we glimpse at objects of interest when we talk about them, and, without naming them
									explicitly, our conversational partner will know what we are referring to. Furthermore we
									signalize attention or the lack of it, while maintaining eye contact or looking in another
									direction, respectively. Natural conversation is all about sensing these inconspicuous non-
									verbal signs. Consequently a robot needs to have these information, in order to make the
									human-robot interaction as natural as talking to another human.</p>
									<p>In 2010, Reale, Hung and Yin published a paper entitled “Viewing Direction Estimation
									Based on 3-D Eyeball Construction for HRI”, where they describe a vision-based approach
									to estimate the gaze of a person under the aspect of human-robot interaction, that is able
									to work with normal webcams. This Study Thesis first describes this approach in detail
									and discusses potential aspects of improvement.</p>
									<p>The gaze estimation approach has been implemented and evaluated and finally ideas for
									enhancements and future work will be collected.</p>
					        <strong>Keywords: </strong>Gaze Estimation, Eyeball Model</br>
					      </div>
						    </p>
						  </div>
						  </li>

							<!-- END OF LIST OF PUBLICATIONS -->

						  </ul>

						</div>
					</section>



			<!-- Photography -->
					<section id="photography" class="three">
						<div class="container">
					
							<header>
								<h2>Photography</h2>
							</header>
							
							<p>Besides Computer Vision research, I'm also interested in taking pictures and computational photography, such as panorama stitching and high-dynamic-range imaging. </p>
						
							<div class="row">
								<div class="4u">
									
									<article class="item">
										<a href="https://plus.google.com/photos/108898617014748189860/albums/5979466675404712913/5979466679246422930?pid=5979466679246422930&oid=108898617014748189860" class="image full"><img src="images/photography/hoy.jpg" alt="" /></a>
										<header>
											<h3>Hoy Island in Lake Constance (Bodensee)</h3>
										</header>
									</article>

									<article class="item">
										<a href="https://plus.google.com/photos/108898617014748189860/albums/5979466675404712913/5979466686709843154?pid=5979466686709843154&oid=108898617014748189860" class="image full"><img src="images/photography/seilbahn.jpg" alt="" /></a>
										<header>
											<h3>Teleferic, Bregenz</h3>
										</header>
									</article>
								</div>

								<div class="4u">
									<article class="item">
										<a href="https://plus.google.com/photos/108898617014748189860/albums/5655667567708027345/5657857363527693794?pid=5657857363527693794&oid=108898617014748189860" class="image full"><img src="images/photography/scotland1.jpg" alt="" /></a>
										<header>
											<h3>Scottish Highlands</h3>
										</header>
									</article>
									<article class="item">
										<a href="https://plus.google.com/photos/108898617014748189860/albums/5655667567708027345/5657857372342932594?pid=5657857372342932594&oid=108898617014748189860" class="image full"><img src="images/photography/scotland2.jpg" alt="" /></a>
										<header>
											<h3>Lonely house in Scotland</h3>
										</header>
									</article>
								</div>

								<div class="4u">
									<article class="item">
										<a href="https://plus.google.com/photos/108898617014748189860/albums/5706747168309694913/5706747226798653938?pid=5706747226798653938&oid=108898617014748189860" class="image full"><img src="images/photography/wintergarten.jpg" alt="" /></a>
										<header>
											<h3>Karlsruhe botanical garden in spring</h3>
										</header>
									</article>
									<article class="item">
										<a href="https://plus.google.com/photos/108898617014748189860/albums/5706747168309694913/5706747205308116066?pid=5706747205308116066&oid=108898617014748189860" class="image full"><img src="images/photography/sphinxes.jpg" alt="" /></a>
										<header>
											<h3>... and in winter</h3>
										</header>
									</article>
								</div>
							</div>

						</div>
					</section>


			
				<!-- Contact -->
					<section id="contact" class="four">
						<div class="container">

							<header>
								<h2>Contact</h2>
							</header>

							<p>If you have any questions, requests, or information to share, please don't hesitate to contact me.
							I'll do my best to answer as quickly as possible.</p>

							You can reach me via</br>
							<a href="mailto:timo.schneider@student.kit.edu">timo.schneider@student.kit.edu</a>
							<ul class="icons big">
							<!--<li><a href="#" class="fa fa-twitter solo"><span>Twitter</span></a></li>--> 
							<!--<li><a href="#" class="fa fa-facebook solo"><span>Facebook</span></a></li> Sorry, I'm not on facebook ;) -->
							<li><a href="https://github.com/tschneid" class="fa fa-github solo"><span>Github</span></a></li>
							<li><a href="https://plus.google.com/+TimoSchneider1" class="fa fa-google-plus solo"><span>Google+</span></a></li>
							<li><a href="https://www.xing.com/profile/Timo_Schneider82" class="fa fa-xing solo"><span>Xing</span></a></li>
							<li><a href="https://www.linkedin.com/profile/view?id=230909248" class="fa fa-linkedin solo"><span>LinkedIn</span></a></li>
							</ul>
							
							<!--<form method="post" action="#">
								<div class="row half">
									<div class="6u"><input type="text" class="text" name="name" placeholder="Name" /></div>
									<div class="6u"><input type="text" class="text" name="email" placeholder="Email" /></div>
								</div>
								<div class="row half">
									<div class="12u">
										<textarea name="message" placeholder="Message"></textarea>
									</div>
								</div>
								<div class="row">
									<div class="12u">
										<a href="#" class="button default">Send Message</a>
									</div>
								</div>
							</form>-->

						</div>
					</section>
			
			</div>

		<!-- Footer -->
			<div id="footer">
				
				<!-- Copyright -->
					<div class="copyright">
						<p>&copy; 2014 Timo Schneider. All rights reserved.</p>
						<ul class="menu">
							<li>Hosted on <a href="https://pages.github.com/">GitHub Pages</a> &ndash; Design by <a href="http://html5up.net">HTML5 UP</a></li>
						</ul>
					</div>
				
			</div>

    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
      ga('create', 'UA-50460629-1', 'timoschneider.me');
      ga('send', 'pageview');
    </script>

	</body>
</html>